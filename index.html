<!--
  Copyright 2018 The Distill Template Authors
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<style>
.base-grid,
.n-header,
.n-byline,
.n-title,
.n-article,
.n-footer {
    display: grid;
    justify-items: stretch;
    grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
    grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media(min-width: 1000px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media (min-width: 1180px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }
    .grid {
        grid-column-gap: 32px;
    }

}

.base-grid {
  grid-column: screen;
}

/* default grid column assignments */
.n-title > *  {
  grid-column: text;
}

.n-article > *  {
  grid-column: text;
}

.n-title {
    padding: 4rem 0 0.5rem;
}

.l-page {
    grid-column: page;
}

.l-article {
    grid-column: text;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}


.pixelated {
    image-rendering: pixelated;
}

strong {
    font-weight: 600;
}

/*------------------------------------------------------------------*/
/* title */
.n-title h1 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 40px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
    text-align: center;
}

@media (min-width: 768px) {
    .n-title h1 {
        font-size: 50px;
    }
}


.n-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}

.n-byline .byline {
  grid-column: text;
}

.byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

@media (min-width: 768px) {
.grid {
    grid-column-gap: 16px;
}
}

.n-byline p {
  margin: 0;
}

.n-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
}
.n-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

ul.authors {
    list-style-type: none;
    padding: 0;
    margin: 0;
    text-align: center;
}
ul.authors li {
    padding: 0 0.5rem;
    display: inline-block;
}

ul.authors sup {
    color: rgb(126,126,126);
}

ul.authors.affiliations  {
    margin-top: 0.5rem;
}

ul.authors.affiliations li {
    color: rgb(126,126,126);
}



</style>
<head>
    <title>Generate, Annotate, and Learn: NLP with Synthetic Text</title>
    <script src="template.v2.js"></script>
    <meta property="og:title" content="Generate, Annotate, and Learn: NLP with Synthetic Text">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="utf8">
</head>

<body>
  <div class="n-title">
   <h1>
    Generate, Annotate, and Learn: NLP with Synthetic Text <span style="font-size: 22px"><a href = "https://arxiv.org/abs/2106.06168" style="text-decoration:none; color: inherit;"><b>[PDF]</b></a></span>
   </h1>
  </div>
  <div class="n-byline">
   <div class="byline">
    <ul class="authors">
      <a href = "https://xlhex.github.io/" style="text-decoration:none; color: inherit;">Xuanli He</a>
     </li>
     <li>
      <a href = "https://scholar.google.com.au/citations?user=WUrsctAAAAAJ&hl=en" style="text-decoration:none; color: inherit;">Islam Nassar</a>
     </li>
     <li>
      <a href = "https://www.cs.toronto.edu/~rkiros/" style="text-decoration:none; color: inherit;">Jamie Kiros</a>
     </li>
     <li>
      <a href = "https://users.monash.edu.au/~gholamrh/" style="text-decoration:none; color: inherit;">Gholamreza Haffari</a>
     </li>
     <li>
      <a href = "https://norouzi.github.io/" style="text-decoration:none; color: inherit;">Mohammad Norouzi</a>
    </ul>
   </div>
  </div>
  <d-article>
    <p>
    We've looked into the use of large language models (LLMs) for generating synthetic text for NLP. We’ve improved the few-shot learning performance of <a href="https://openai.com/blog/gpt-3-apps">GPT-3 6B</a> by conditioning the GPT-3 model on a few input-output examples, and using it to generate new synthetic input-output examples, which can give the model more context. In addition, we’ve found that using synthetically generated text to distill the knowledge of a compute-intensive transformer into a compact transformer leads to state-of-the-art performance for efficient NLP models on the <a href="https://gluebenchmark.com/">GLUE benchmark</a>. We call our approach GAL: <strong>G</strong>enerate, <strong>A</strong>nnotate and <strong>L</strong>earn
    </p>


    <!--<figure  style="grid-column: text">-->
        <!--<img src="images/human_eval.png" style="width: 100%; margin-top: 1rem;"/>-->
          <!--<figcaption> We conduct 2-Alternatative Forced Choice Experiment human evaluation experiment. Subjects are asked to choose between reference high resolution image, and the model output. We measure the performance of the model through confusion rates (% of time, raters choose model output over reference images.) (Above) We achieve close to 50% confusion rate on the task of 16&#215;16 -> 128&#215;128 faces outperforming state of the art face super-resolution methods. (Below) We also achieve 40% confusion rate on the much difficult task of 64x64 -> 256x256 natural images outperforming regression baseline by a large margin. </figcaption>-->
    <!--</figure>-->


<h3>Few-shot Learning with Synthetic Text </h3>
<p>
We first test our approach on few-shot learning with GPT-3 6B. GPT-3 proposed a novel approach to conduct few-shot learning. Specifically, as shown in Figure 1, one can craft a prompt consisting of instruction, a few labeled instances, and a new unlabeled text example. GPT-3 will complete the prompt by generating the corresponding label for the unlabeled instance.
</p> 
      <figure  style="grid-column: text">
          <img src="images/fewshot.gif" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> <b>Figure 1</b>: Prompt-based few-shot learning.  </figcaption>
      </figure>
<p>
Now, we apply GAL to prompt-based few-shot learning. According to Figure 2, we present k labeled examples as a prompt to GPT-3 6B, and generate m synthetic examples, followed by the corresponding labels. Note that to mitigate noisy outputs, the generation of each synthetic example only conditions on the original k labeled examples. Finally, we concatenate the original k examples and m synthetic examples, and conduct a (k+m)-shot learning experiment with GPT-3 6B.
</p> 
      <figure  style="grid-column: text">
          <img src="images/gal_fewshot.gif" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> <b>Figure 2</b>: Prompt-based few-shot learning with GAL.  </figcaption>
      </figure>

      <p>
      Figure 3 shows that GAL can significantly improve the few-shot learning results by generating more synthetic labeled examples.
      </p>
      <figure  style="grid-column: text">
          <img src="images/fewshot_results.svg" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> <b>Figure 3</b>: Comparison between standard few-shot learning and GAL.  </figcaption>
      </figure>

    <h3>Knowledge Distillation with Synthetic Text</h3>
    
    <p>
There is an abundance of unlabeled data in the real world, but task-specific unlabeled data can be challenging to find. For instance, one cannot find in-domain unlabeled text conforming to the input distribution of a specific NLP task from the <a href="https://gluebenchmark.com/">GLUE benchmark</a>. Some NLP tasks require an input comprising a pair of sentences with a particular relationship. If task-specific unlabeled data were available, we could use them knowledge distillation. To fill in this gap, we consider using LLMs as a means of data synthesizer. As shown in Figure 4, we first finetune a GPT2<d-cite key="radford2019language"></d-cite> model on the in-domain data without specified labels to steer LLMs towards generating domain-specific unlabeled data. Afterward, we can generate many in-domain unlabeled data via the tailored GPT2.
    </p>
    <figure  style="grid-column: text">
        <!--<img src="images/overview.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>-->
        <img src="images/overview.png" style="width: 100%; margin-top: 1rem;"/>
          <figcaption style="display:flex; justify-content: center">  <b>Figure 4</b>: Synthesizing in-domain data by fine-tuning GPT2  </figcaption>
    </figure>

      <p>
      We first train a teacher model from the labeled training data. Then we can annotate the unlabeled data with the aid of the teacher model. Eventually, we can compress the knowledge of the teacher model into a compact student model via the original training data and pseudo-labeled synthetic data. 
We compare GAL with the following baselines:  BERT-Theseus<d-cite key="xu2020bert"></d-cite>, tinyBERT<d-cite key="jiao2019tinybert"></d-cite>, MATE-KD<d-cite key="rashid2021mate"></d-cite>, DistilRoBERTa<d-cite key="Sanh2019DistilBERTAD"></d-cite>, and DistilRoBERTa + RT (round-trip translation).
      </p>

      <figure  style="grid-column: text">
          <img src="images/kd_results.svg" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="justify-content: center"> <b>Figure 5</b>: GLUE test results (average over 8 tasks) for a 6-layer transformer.  GAL establishes a new state-of-the-art on KD for NLP.  </figcaption>
      </figure>

<h3>GAL for Other Classification Tasks </h3>
<p>
To demonstrate the universality of GAL, we also employ it for tabular and image classification tasks. According to Figure 6 and 7, GAL is effective on tabular and image classification tasks as well.
</p>
      <figure  style="grid-column: text">
          <img src="images/cifar10_results.svg" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="justify-content: center"> <b>Figure 6</b>:  Classification error rates on CIFAR-10 (an image classification task) test set with varying amounts of synthetic data for three different model architectures. </figcaption>
      </figure>
      
      <figure  style="grid-column: text">
          <img src="images/tabular.svg" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="justify-content: center"> <b>Figure 7</b>: RoBERTa-base and GAL results on four tabular datasets from the UCI repository.   Accuracy is reported for these datasets. </figcaption>
      </figure>

  <h3>Synthetic Examples</h3>
  <p>
  We provide some synthetic images and text generated by our approach below. Please refer to our <a href="https://arxiv.org/abs/2106.06168">paper</a> for more examples.
  </p>
  <h4> Synthetic Images</h4>
      <figure  style="grid-column: text">
          <img src="images/gen_img.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="justify-content: center"> <b>Figure 8</b>: CIFAR-10 synthetic samples generated by NCSN and corresponding pseudo-labels. Images are filtered based on a confidence threshold of &tau;=0.95 and categorized based on pseudo-labels. For each category, 16 random samples are shown.</figcaption>
      </figure>
  <h4> Synthetic Text</h4>
      <figure  style="grid-column: text">
          <img src="images/gen_text.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="justify-content: center"> <b>Figure 9</b>: Two labeled examples from QNLI, along with 3 nearest neighbors (based on RoBERTa representations) from our synthetic dataset. We include labels for original examples and pseudo-labels for synthetic examples in parenthesis.</figcaption>
      </figure>
      
      

<h3>Citation</h3>
<p>
  For more details and additional results, <a href="https://arxiv.org/abs/2106.06168">read the full paper</a>.
</p>
<code>
  @article{he2021generate,
  <div style="padding-left: 5%; margin: 0">
    title={Generate, annotate, and learn: Generative models advance self-training and knowledge distillation},<br/>
    author={He, Xuanli and Nassar, Islam and Kiros, Jamie and Haffari, Gholamreza and Norouzi, Mohammad},<br/>
    journal={arXiv:2106.06168},<br/>
    year={2021}}<br/>
  </div>
  }<br/>
</code>
<span style="margin-bottom: 10%"></span>
</d-article>

<d-appendix>
      <!--<d-bibliography src="https://raw.githubusercontent.com/iterative-refinement/iterative-refinement.github.io/master/bibliography.bib"></d-bibliography>-->
      <d-bibliography src="https://raw.githubusercontent.com/galtext/galtext.github.io/master/bibliography.bib"></d-bibliography>
</d-appendix>

</body>
