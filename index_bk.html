<!--
  Copyright 2018 The Distill Template Authors
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<style>
.base-grid,
.n-header,
.n-byline,
.n-title,
.n-article,
.n-footer {
    display: grid;
    justify-items: stretch;
    grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
    grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media(min-width: 1000px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media (min-width: 1180px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }
    .grid {
        grid-column-gap: 32px;
    }

}

.base-grid {
  grid-column: screen;
}

/* default grid column assignments */
.n-title > *  {
  grid-column: text;
}

.n-article > *  {
  grid-column: text;
}

.n-title {
    padding: 4rem 0 0.5rem;
}

.l-page {
    grid-column: page;
}

.l-article {
    grid-column: text;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}


.pixelated {
    image-rendering: pixelated;
}

strong {
    font-weight: 600;
}

/*------------------------------------------------------------------*/
/* title */
.n-title h1 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 40px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
    text-align: center;
}

@media (min-width: 768px) {
    .n-title h1 {
        font-size: 50px;
    }
}


.n-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}

.n-byline .byline {
  grid-column: text;
}

.byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

@media (min-width: 768px) {
.grid {
    grid-column-gap: 16px;
}
}

.n-byline p {
  margin: 0;
}

.n-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
}
.n-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

ul.authors {
    list-style-type: none;
    padding: 0;
    margin: 0;
    text-align: center;
}
ul.authors li {
    padding: 0 0.5rem;
    display: inline-block;
}

ul.authors sup {
    color: rgb(126,126,126);
}

ul.authors.affiliations  {
    margin-top: 0.5rem;
}

ul.authors.affiliations li {
    color: rgb(126,126,126);
}

figcaption {
  color: white;
  padding: 2px;
  text-align: center;
}


</style>
<head>
    <title>Generate, Annotate, and Learn: NLP with Synthetic Text</title>
    <script src="template.v2.js"></script>
    <meta property="og:title" content="Generate, Annotate, and Learn: NLP with Synthetic Text">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="utf8">
</head>

<body>
  <div class="n-title">
   <h1>
    Generate, Annotate, and Learn: NLP with Synthetic Tex&nbsp; <span style="font-size: 22px"><a href = "https://arxiv.org/abs/2106.06168" style="text-decoration:none; color: inherit;"><b>[PDF]</b></a></span>
   </h1>
  </div>
  <div class="n-byline">
   <div class="byline">
    <ul class="authors">
      <a href = "https://xlhex.github.io/" style="text-decoration:none; color: inherit;">Xuanli He</a>
     </li>
     <li>
      <a href = "https://scholar.google.com.au/citations?user=WUrsctAAAAAJ&hl=en" style="text-decoration:none; color: inherit;">Islam Nassar</a>
     </li>
     <li>
      <a href = "https://www.cs.toronto.edu/~rkiros/" style="text-decoration:none; color: inherit;">Jamie Kiros</a>
     </li>
     <li>
      <a href = "https://users.monash.edu.au/~gholamrh/" style="text-decoration:none; color: inherit;">Gholamreza Haffari</a>
     </li>
     <li>
      <a href = "https://norouzi.github.io/" style="text-decoration:none; color: inherit;">Mohammad Norouzi</a>
    </ul>
   </div>
  </div>
  <d-article>
    <video controls autoplay loop width="1080" height="560" style="object-fit: contain;grid-column: page;">
     <source src="videos/gal.mp4" type="video/mp4">
     </video>
    <h3>Summary</h3>
    <p>
   We present <strong>G</strong>enerate, <strong>A</strong>nnotate and <strong>L</strong>earn (GAL), an approach to advancing self-training, knowledge distillation (KD), and prompt-based few-shot learning with the synthetic text. GAL leverages large pre-trained language models (LLMs) to synthesize many in-domain unlabeled data. An external classifier can annotate these unlabeled data. We manage to improve the performance of self-training and KD via the human- and machine-annotated data. On self-training, GAL can surpass a strong baseline (RoBERTa-base) by 1.5% on GLUE benchmark, whereas we push the KD performance to 84.8%, marking a new SOTA for 6-layer transformer on GLUE benchmark. Besides, our approach further improves prompt-based few-shot learning, providing an average improvement of 1.3% on four 4-shot learning NLP tasks, outperforming GPT-3-6B. Finally, we demonstrate that our approach can be applied to tabular and image classification tasks, yielding an average improvement of 1.5% and 0.8% respectively.
    </p>
    <h3>Text Synthesization via GPT2</h3>
    <p>
There is an abundance of unlabeled data in the real world, but task-specific unlabeled data can be challenging to find. For instance, one cannot find in-domain unlabeled text conforming to the input distribution of a specific NLP task from the <a href="https://gluebenchmark.com/">GLUE benchmark</a>. Some NLP tasks require an input comprising a pair of sentences with a particular relationship. If task-specific unlabeled data were available, we could use them for self-training and knowledge distillation. To fill in this gap, we consider using LLMs as a means of data synthesizer. As shown in Figure 1, we first finetune a GPT2 model on the in-domain data without specified labels to steer LLMs towards generating domain-specific unlabeled data. Afterward, we can generate many in-domain unlabeled data via the tailored GPT2.
    </p>
    <figure  style="grid-column: text">
        <!--<img src="images/overview.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>-->
        <img src="images/overview.png" style="width: 100%; margin-top: 1rem;"/>
          <figcaption>  <b>Figure 1</b>: Synthesizing in-domain data by fine-tuning GPT2  </figcaption>
    </figure>


    <!--<figure  style="grid-column: text">-->
        <!--<img src="images/human_eval.png" style="width: 100%; margin-top: 1rem;"/>-->
          <!--<figcaption> We conduct 2-Alternatative Forced Choice Experiment human evaluation experiment. Subjects are asked to choose between reference high resolution image, and the model output. We measure the performance of the model through confusion rates (% of time, raters choose model output over reference images.) (Above) We achieve close to 50% confusion rate on the task of 16&#215;16 -> 128&#215;128 faces outperforming state of the art face super-resolution methods. (Below) We also achieve 40% confusion rate on the much difficult task of 64x64 -> 256x256 natural images outperforming regression baseline by a large margin. </figcaption>-->
    <!--</figure>-->

    <p>
      <h3>GAL for Self-training and Knowledge Distillation</h3>
      <p>
      Once the synthetic unlabeled data is available, we can apply self-training. We train a classifier on the labeled training data; then, the unlabeled data can be pseudo-labeled by this classifier. We can obtain an enhanced classifier using the original training data and pseudo-labeled synthetic data. This process can be iterated multiple times until the performance reaches a plateau.
      </p>
    <figure  style="grid-column: text">
        <img src="images/self_training.png" style="width: 100%; margin-top: 1rem;"/>
          <figcaption>  <b>Table 1</b>: RoBERTa base and GAL self-training results on GLUE dev sets </figcaption>
    </figure>
      <p>
      Similarly, we first train a teacher model from the labeled training data. Then we can annotate the unlabeled data with the aid of the teacher model. Eventually, we can compress the knowledge of the teacher model into a compact student model via the original training data and pseudo-labeled synthetic data. test
      <!--<d-cite key="du2020self"></d-cite>-->
      <d-cite key="goodfellow2014generative"></d-cite>
      <!--<d-cite key="goodfellow2014generative, radford2015unsupervised"></d-cite> -->
      </p>

      <figure  style="grid-column: text">
          <img src="images/kd.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> <b>Table 2</b>: GLUE test results for a 6-layer transformer.  GAL establishes a new state-of-the-art on KD for NLP.  </figcaption>
      </figure>

      <figure  style="grid-column: text">
          <img src="images/unconditional_faces.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> Selected example generations of unconditional 1024&#215;1024 faces.  </figcaption>
      </figure>

      <figure  style="grid-column: text">
          <img src="images/class_cond_images.png" style="width: 100%; margin-top: 1rem;display: block; margin-left: auto; margin-right: auto;"/>
          <figcaption style="display:flex; justify-content: center"> Selected example generations of class conditional 256&#215;256 natural images. Each row contains examples from a particular class.  </figcaption>
      </figure>

<h3>GAL for Prompt-based Few-shot Learning</h3>
<p>
In addition to the fine-tuning paradigm, we also test GAL on prompt-based few-shot learning, where one can leverage LLMs to correctly predict the inputs' labels by conditioning on a prompt, which consists of an instruction, a few labeled instances, and a new unlabeled input. We apply GAL to prompt-based few-shot learning. Specifically, we present k labeled examples as a prompt to GPT-J, an open-sourced re-implementation of GPT-3-6B, and generate m synthetic examples, followed by the corresponding labels. Note that to mitigate noisy outputs, the generation of each synthetic example only conditions on the original k labeled examples. Finally, we concatenate the original k examples and m synthetic examples, and conduct a (k+m)-shot learning experiment with GPT-J.
</p> 

<h4>GAL for Other Classification Tasks </h4>
<p>
To demonstrate the universality of GAL, we also employ it for tabular and image classification tasks. According to Table 4 and 5, GAL is effective on tabular and image classification tasks as well.
</p>
      

<h3>Citation</h3>
<p>
  For more details and additional results, <a href="https://arxiv.org/abs/2104.07636">read the full paper</a>.
</p>
<code>
  @article{saharia2021image,
  <div style="padding-left: 5%; margin: 0">
    title={Image super-resolution via iterative refinement},<br/>
    author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J and Norouzi, Mohammad},<br/>
    journal={arXiv:2104.07636},<br/>
    year={2021}}<br/>
  </div>
  }<br/>
</code>
<span style="margin-bottom: 10%"></span>
</d-article>

<d-appendix>
      <!--<d-bibliography src="https://raw.githubusercontent.com/iterative-refinement/iterative-refinement.github.io/master/bibliography.bib"></d-bibliography>-->
      <d-bibliography src="https://raw.githubusercontent.com/galtext/galtext.github.io/master/bibliography.bib"></d-bibliography>
</d-appendix>

</body>
